[2024-10-21T13:09:44.809+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-10-21T13:09:44.830+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_webscraping.kabum_tasks.Spark_Transformation_kabum manual__2024-10-21T13:03:12.581641+00:00 [queued]>
[2024-10-21T13:09:44.838+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_webscraping.kabum_tasks.Spark_Transformation_kabum manual__2024-10-21T13:03:12.581641+00:00 [queued]>
[2024-10-21T13:09:44.838+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2024-10-21T13:09:44.853+0000] {taskinstance.py:2888} INFO - Executing <Task(OperatorSubmitSpark): kabum_tasks.Spark_Transformation_kabum> on 2024-10-21 13:03:12.581641+00:00
[2024-10-21T13:09:44.859+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2026) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-10-21T13:09:44.861+0000] {standard_task_runner.py:72} INFO - Started process 2028 to run task
[2024-10-21T13:09:44.861+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'pipeline_webscraping', 'kabum_tasks.Spark_Transformation_kabum', 'manual__2024-10-21T13:03:12.581641+00:00', '--job-id', '303', '--raw', '--subdir', 'DAGS_FOLDER/DagWebScraping.py', '--cfg-path', '/tmp/tmplwe5mswi']
[2024-10-21T13:09:44.864+0000] {standard_task_runner.py:105} INFO - Job 303: Subtask kabum_tasks.Spark_Transformation_kabum
[2024-10-21T13:09:44.904+0000] {task_command.py:467} INFO - Running <TaskInstance: pipeline_webscraping.kabum_tasks.Spark_Transformation_kabum manual__2024-10-21T13:03:12.581641+00:00 [running]> on host e2c3a206786d
[2024-10-21T13:09:44.997+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='pipeline_webscraping' AIRFLOW_CTX_TASK_ID='kabum_tasks.Spark_Transformation_kabum' AIRFLOW_CTX_EXECUTION_DATE='2024-10-21T13:03:12.581641+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-10-21T13:03:12.581641+00:00'
[2024-10-21T13:09:44.998+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-10-21T13:09:45.025+0000] {logging_mixin.py:190} INFO - teste /opt/***/datalake/rawzone/kabum_2024-10-21/data.json
[2024-10-21T13:09:45.025+0000] {baseoperator.py:405} WARNING - OperatorSubmitSpark.execute cannot be called outside TaskInstance!
[2024-10-21T13:09:45.032+0000] {spark_submit.py:304} INFO - Could not load connection string spark_default, defaulting to yarn
[2024-10-21T13:09:45.034+0000] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master yarn --num-executors 2 --driver-memory 512M --name arrow-spark /opt/bitnami/spark/src/scripts_spark/WebScraping_Transformation.py --path_save_transfor /opt/***/datalake/silverzone/kabum_2024-10-21 --json_file_path /opt/***/datalake/rawzone/kabum_2024-10-21/data.json
[2024-10-21T13:09:48.421+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkContext: Running Spark version 3.5.2
[2024-10-21T13:09:48.423+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2024-10-21T13:09:48.424+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkContext: Java version 17.0.12
[2024-10-21T13:09:48.475+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-10-21T13:09:48.542+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO ResourceUtils: ==============================================================
[2024-10-21T13:09:48.542+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-10-21T13:09:48.543+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO ResourceUtils: ==============================================================
[2024-10-21T13:09:48.543+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkContext: Submitted application: Test
[2024-10-21T13:09:48.559+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-10-21T13:09:48.566+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO ResourceProfile: Limiting resource is cpu
[2024-10-21T13:09:48.567+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-10-21T13:09:48.606+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SecurityManager: Changing view acls to: ***
[2024-10-21T13:09:48.607+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SecurityManager: Changing modify acls to: ***
[2024-10-21T13:09:48.608+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SecurityManager: Changing view acls groups to:
[2024-10-21T13:09:48.608+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SecurityManager: Changing modify acls groups to:
[2024-10-21T13:09:48.609+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-10-21T13:09:48.778+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO Utils: Successfully started service 'sparkDriver' on port 36281.
[2024-10-21T13:09:48.800+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkEnv: Registering MapOutputTracker
[2024-10-21T13:09:48.826+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkEnv: Registering BlockManagerMaster
[2024-10-21T13:09:48.838+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-10-21T13:09:48.839+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-10-21T13:09:48.842+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-10-21T13:09:48.858+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dcc3170b-7a01-4860-b7cd-9a263943e902
[2024-10-21T13:09:48.869+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO MemoryStore: MemoryStore started with capacity 127.2 MiB
[2024-10-21T13:09:48.884+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-10-21T13:09:48.980+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:48 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-10-21T13:09:49.025+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-10-21T13:09:49.110+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO Executor: Starting executor ID driver on host e2c3a206786d
[2024-10-21T13:09:49.111+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO Executor: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2024-10-21T13:09:49.111+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO Executor: Java version 17.0.12
[2024-10-21T13:09:49.117+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-10-21T13:09:49.118+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@334692d1 for default.
[2024-10-21T13:09:49.134+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37869.
[2024-10-21T13:09:49.134+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO NettyBlockTransferService: Server created on e2c3a206786d:37869
[2024-10-21T13:09:49.135+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-10-21T13:09:49.140+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e2c3a206786d, 37869, None)
[2024-10-21T13:09:49.143+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO BlockManagerMasterEndpoint: Registering block manager e2c3a206786d:37869 with 127.2 MiB RAM, BlockManagerId(driver, e2c3a206786d, 37869, None)
[2024-10-21T13:09:49.145+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e2c3a206786d, 37869, None)
[2024-10-21T13:09:49.146+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e2c3a206786d, 37869, None)
[2024-10-21T13:09:49.474+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-10-21T13:09:49.479+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:49 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-10-21T13:09:50.101+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:50 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
[2024-10-21T13:09:50.167+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:50 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
[2024-10-21T13:09:51.708+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:51 INFO FileSourceStrategy: Pushed Filters:
[2024-10-21T13:09:51.709+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:51 INFO FileSourceStrategy: Post-Scan Filters:
[2024-10-21T13:09:51.914+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.9 KiB, free 127.0 MiB)
[2024-10-21T13:09:51.973+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 127.0 MiB)
[2024-10-21T13:09:51.976+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.2 MiB)
[2024-10-21T13:09:51.981+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:51 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:51.990+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:52.118+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:52.131+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:52.132+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:52.133+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:52.133+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:52.136+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:52.199+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.2 KiB, free 127.0 MiB)
[2024-10-21T13:09:52.205+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 126.9 MiB)
[2024-10-21T13:09:52.205+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e2c3a206786d:37869 (size: 7.6 KiB, free: 127.2 MiB)
[2024-10-21T13:09:52.207+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:52.221+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:52.223+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-10-21T13:09:52.257+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:52.268+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-10-21T13:09:52.348+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:52.527+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO CodeGenerator: Code generated in 152.867844 ms
[2024-10-21T13:09:52.635+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2073 bytes result sent to driver
[2024-10-21T13:09:52.646+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 397 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:52.647+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-10-21T13:09:52.661+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0.513 s
[2024-10-21T13:09:52.664+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:52.664+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-10-21T13:09:52.666+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:52 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0.547707 s
[2024-10-21T13:09:53.129+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,placas-mae)
[2024-10-21T13:09:53.130+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = placas-mae)
[2024-10-21T13:09:53.309+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO CodeGenerator: Code generated in 64.478666 ms
[2024-10-21T13:09:53.313+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.7 KiB, free 126.8 MiB)
[2024-10-21T13:09:53.326+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.7 MiB)
[2024-10-21T13:09:53.327+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:53.329+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:53.332+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:53.362+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.2 MiB)
[2024-10-21T13:09:53.365+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:53.366+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:53.367+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:53.367+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:53.368+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:53.368+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:53.380+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 29.7 KiB, free 126.9 MiB)
[2024-10-21T13:09:53.384+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 126.9 MiB)
[2024-10-21T13:09:53.386+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e2c3a206786d:37869 (size: 11.0 KiB, free: 127.1 MiB)
[2024-10-21T13:09:53.386+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:53.387+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:53.387+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-10-21T13:09:53.388+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:53.389+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-10-21T13:09:53.420+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on e2c3a206786d:37869 in memory (size: 7.6 KiB, free: 127.2 MiB)
[2024-10-21T13:09:53.464+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO CodeGenerator: Code generated in 45.531407 ms
[2024-10-21T13:09:53.467+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:53.481+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO CodeGenerator: Code generated in 11.037759 ms
[2024-10-21T13:09:53.502+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO CodeGenerator: Code generated in 5.12686 ms
[2024-10-21T13:09:53.594+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3336 bytes result sent to driver
[2024-10-21T13:09:53.596+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 208 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:53.597+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.224 s
[2024-10-21T13:09:53.598+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:53.598+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-10-21T13:09:53.599+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-10-21T13:09:53.599+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.233183 s
[2024-10-21T13:09:53.629+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO CodeGenerator: Code generated in 17.281875 ms
[2024-10-21T13:09:53.639+0000] {spark_submit.py:634} INFO - +---------+-----+--------+---+-------+--------------------+------------+----------+
[2024-10-21T13:09:53.640+0000] {spark_submit.py:634} INFO - |name_mark|model|  socket|DDR|  value|                 uri|data_collect|  category|
[2024-10-21T13:09:53.640+0000] {spark_submit.py:634} INFO - +---------+-----+--------+---+-------+--------------------+------------+----------+
[2024-10-21T13:09:53.640+0000] {spark_submit.py:634} INFO - |     ASUS| A520|     AM4|  4| 599.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.641+0000] {spark_submit.py:634} INFO - | GIGABYTE| B550|     AM4|  4| 829.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.641+0000] {spark_submit.py:634} INFO - |      MSI| B550|     AM4|  0| 859.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.641+0000] {spark_submit.py:634} INFO - |      MSI| A520|     AM4|  4| 374.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.642+0000] {spark_submit.py:634} INFO - |      MSI| B550|     AM4|  0| 999.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.642+0000] {spark_submit.py:634} INFO - | GIGABYTE| H610| LGA1700|  4| 549.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.642+0000] {spark_submit.py:634} INFO - |     ASUS| B550|     AM4|  4| 899.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.643+0000] {spark_submit.py:634} INFO - |   ASROCK| B450|     AM4|  4| 769.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.643+0000] {spark_submit.py:634} INFO - |     ASUS| H610|LGA 1700|  4| 679.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.643+0000] {spark_submit.py:634} INFO - |     ASUS| H510|LGA 1200|  4| 504.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.644+0000] {spark_submit.py:634} INFO - | GIGABYTE| B650|     AM5|  5| 919.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.644+0000] {spark_submit.py:634} INFO - |     ASUS| B550|     AM4|  4| 807.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.644+0000] {spark_submit.py:634} INFO - |     ASUS| A520|     AM4|  4| 484.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.645+0000] {spark_submit.py:634} INFO - |      MSI| B450|     AM4|  4| 634.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.645+0000] {spark_submit.py:634} INFO - |     ASUS| B650|     AM5|  5|1189.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.646+0000] {spark_submit.py:634} INFO - |   ASROCK| B660|LGA 1700|  4| 869.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.646+0000] {spark_submit.py:634} INFO - |      MSI| B450|     AM4|  4| 494.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.647+0000] {spark_submit.py:634} INFO - |      MSI| B560|LGA 1200|  4| 564.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.647+0000] {spark_submit.py:634} INFO - |     ASUS| A620|     AM5|  5|1014.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.648+0000] {spark_submit.py:634} INFO - |     ASUS| B450|     AM4|  4| 709.99|https://www.kabum...|  2024-10-21|placas-mae|
[2024-10-21T13:09:53.648+0000] {spark_submit.py:634} INFO - +---------+-----+--------+---+-------+--------------------+------------+----------+
[2024-10-21T13:09:53.649+0000] {spark_submit.py:634} INFO - only showing top 20 rows
[2024-10-21T13:09:53.650+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:53.885+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,memoria-ram)
[2024-10-21T13:09:53.886+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = memoria-ram)
[2024-10-21T13:09:53.918+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO BlockManagerInfo: Removed broadcast_3_piece0 on e2c3a206786d:37869 in memory (size: 11.0 KiB, free: 127.2 MiB)
[2024-10-21T13:09:53.971+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO CodeGenerator: Code generated in 43.710914 ms
[2024-10-21T13:09:53.974+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.7 KiB, free 126.8 MiB)
[2024-10-21T13:09:53.987+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.7 MiB)
[2024-10-21T13:09:53.989+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:54.000+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:54.002+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:54.019+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:54.020+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:54.021+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:54.021+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:54.022+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:54.022+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:54.026+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 37.7 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.031+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.033+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on e2c3a206786d:37869 (size: 13.6 KiB, free: 127.1 MiB)
[2024-10-21T13:09:54.035+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:54.036+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:54.037+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-10-21T13:09:54.038+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:54.038+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-10-21T13:09:54.097+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO CodeGenerator: Code generated in 42.220739 ms
[2024-10-21T13:09:54.098+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:54.185+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.2 MiB)
[2024-10-21T13:09:54.214+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4592 bytes result sent to driver
[2024-10-21T13:09:54.217+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 179 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:54.218+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.195 s
[2024-10-21T13:09:54.221+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:54.221+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-10-21T13:09:54.222+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-10-21T13:09:54.222+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.201176 s
[2024-10-21T13:09:54.237+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO CodeGenerator: Code generated in 8.944053 ms
[2024-10-21T13:09:54.250+0000] {spark_submit.py:634} INFO - +--------------------+---------+----+---+---+-------+------+--------------------+------------+-----------+
[2024-10-21T13:09:54.251+0000] {spark_submit.py:634} INFO - |         memory_name|name_mark| MHZ| GB|DDR|latency| value|                 uri|data_collect|   category|
[2024-10-21T13:09:54.252+0000] {spark_submit.py:634} INFO - +--------------------+---------+----+---+---+-------+------+--------------------+------------+-----------+
[2024-10-21T13:09:54.252+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM KINGS...| KINGSTON|3200|  8|  4|     16|159.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.255+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM GAMER...|    HUSKY|3200|  8|  4|     19|124.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.255+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM KINGS...| KINGSTON|3200| 16|  4|     16|273.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.256+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM XPG S...|      XPG|3200| 16|  4|     16|279.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.256+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM RISE ...|RISE MODE|1600|  8|  3|     11| 64.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.256+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM XPG G...|      XPG|3200|  8|  4|     16|134.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.257+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM GAMER...|    HUSKY|3200| 16|  4|     19|219.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.257+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM KINGS...| KINGSTON|3200| 16|  4|     16|324.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.257+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM PARA ...|RISE MODE|1600|  8|  3|     11| 59.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.258+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM 8GB K...| NOT_INFO|   0|  8|  4|     22|119.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.258+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM XPG G...|      XPG|3200| 16|  4|     16|244.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.258+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM PARA ...|   KABUM!|2666|  8|  4|     19|104.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.259+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM 16GB ...| NOT_INFO|   0| 16|  4|     22|214.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.259+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM HUSKY...|    HUSKY|2666|  8|  4|     19|119.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.260+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM PARA ...| KINGSTON|3200| 16|  4|     20|309.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.261+0000] {spark_submit.py:634} INFO - |MEMORIA RAM PARA ...|RISE MODE|2666| 16|  4|     19|219.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.262+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM RISE ...|RISE MODE|3200|  8|  4|     19|134.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.262+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM RISE ...|RISE MODE|3200|  8|  4|     19|143.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.263+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM PARA ...| KINGSTON|3200|  8|  4|     20|167.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.263+0000] {spark_submit.py:634} INFO - |MEMÓRIA RAM GAMER...|RISE MODE|1600|  8|  3|     11| 59.99|https://www.kabum...|  2024-10-21|memoria-ram|
[2024-10-21T13:09:54.263+0000] {spark_submit.py:634} INFO - +--------------------+---------+----+---+---+-------+------+--------------------+------------+-----------+
[2024-10-21T13:09:54.264+0000] {spark_submit.py:634} INFO - only showing top 20 rows
[2024-10-21T13:09:54.264+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:54.500+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,processadores)
[2024-10-21T13:09:54.501+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = processadores)
[2024-10-21T13:09:54.538+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Removed broadcast_5_piece0 on e2c3a206786d:37869 in memory (size: 13.6 KiB, free: 127.2 MiB)
[2024-10-21T13:09:54.565+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO CodeGenerator: Code generated in 26.140424 ms
[2024-10-21T13:09:54.567+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 199.7 KiB, free 126.8 MiB)
[2024-10-21T13:09:54.578+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.579+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:54.579+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:54.581+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:54.588+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:54.589+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:54.590+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:54.591+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:54.591+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:54.592+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:54.593+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 31.8 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.596+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 11.4 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.603+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on e2c3a206786d:37869 (size: 11.4 KiB, free: 127.1 MiB)
[2024-10-21T13:09:54.604+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:54.604+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:54.605+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-10-21T13:09:54.606+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.2 MiB)
[2024-10-21T13:09:54.606+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:54.606+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2024-10-21T13:09:54.640+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO CodeGenerator: Code generated in 29.41559 ms
[2024-10-21T13:09:54.641+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:54.676+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3717 bytes result sent to driver
[2024-10-21T13:09:54.678+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 73 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:54.679+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-10-21T13:09:54.679+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.088 s
[2024-10-21T13:09:54.680+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:54.681+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-10-21T13:09:54.682+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.091609 s
[2024-10-21T13:09:54.687+0000] {spark_submit.py:634} INFO - +--------------------+-------+--------------+-----+-------+--------------------+------------+-------------+
[2024-10-21T13:09:54.687+0000] {spark_submit.py:634} INFO - |           name_mark|  model|frequency_base|cache|  value|                 uri|data_collect|     category|
[2024-10-21T13:09:54.688+0000] {spark_submit.py:634} INFO - +--------------------+-------+--------------+-----+-------+--------------------+------------+-------------+
[2024-10-21T13:09:54.688+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 7|           4.1|    4|1439.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.689+0000] {spark_submit.py:634} INFO - |PROCESSADOR INTEL...|CORE I5|           2.5|   18| 669.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.689+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           4.6|    4| 847.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.689+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           3.5|   35| 799.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.690+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 7|           3.4|   36|1299.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.690+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           3.7|   11| 698.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.691+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           3.6|   19| 698.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.691+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           3.7|   35| 998.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.692+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 7|           3.8|   20|1159.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.692+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 3|           3.6|    4| 398.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.692+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           3.6|   16| 570.00|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.693+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 7|           3.8|   36|1449.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.693+0000] {spark_submit.py:634} INFO - |PROCESSADOR INTEL...|CORE I7|           3.6|    0|1799.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.694+0000] {spark_submit.py:634} INFO - |PROCESSADOR INTEL...|CORE I5|           2.5|   18| 899.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.694+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           3.6|   11| 569.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.695+0000] {spark_submit.py:634} INFO - |PROCESSADOR INTEL...|CORE I3|           3.7|    6| 599.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.695+0000] {spark_submit.py:634} INFO - |PROCESSADOR INTEL...|CORE I5|           2.9|   12| 579.90|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.695+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           5.1|   38|1448.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.696+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 5|           4.6|   19| 798.01|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.696+0000] {spark_submit.py:634} INFO - |PROCESSADOR AMD R...|RYZEN 9|           3.7|   70|1999.99|https://www.kabum...|  2024-10-21|processadores|
[2024-10-21T13:09:54.697+0000] {spark_submit.py:634} INFO - +--------------------+-------+--------------+-----+-------+--------------------+------------+-------------+
[2024-10-21T13:09:54.697+0000] {spark_submit.py:634} INFO - only showing top 20 rows
[2024-10-21T13:09:54.697+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:54.833+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,placa-de-video-vga)
[2024-10-21T13:09:54.834+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = placa-de-video-vga)
[2024-10-21T13:09:54.845+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Removed broadcast_7_piece0 on e2c3a206786d:37869 in memory (size: 11.4 KiB, free: 127.2 MiB)
[2024-10-21T13:09:54.890+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO CodeGenerator: Code generated in 22.613945 ms
[2024-10-21T13:09:54.893+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 199.7 KiB, free 126.8 MiB)
[2024-10-21T13:09:54.903+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.904+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:54.905+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:54.906+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:54.915+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:54.915+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:54.916+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:54.916+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:54.917+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:54.917+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:54.918+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.9 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.921+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 126.7 MiB)
[2024-10-21T13:09:54.922+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on e2c3a206786d:37869 (size: 11.0 KiB, free: 127.1 MiB)
[2024-10-21T13:09:54.924+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:54.924+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:54.925+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-10-21T13:09:54.925+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:54.925+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2024-10-21T13:09:54.940+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO BlockManagerInfo: Removed broadcast_6_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.2 MiB)
[2024-10-21T13:09:54.958+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO CodeGenerator: Code generated in 27.803508 ms
[2024-10-21T13:09:54.959+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:54.995+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3556 bytes result sent to driver
[2024-10-21T13:09:54.996+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 72 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:54.997+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-10-21T13:09:54.997+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.080 s
[2024-10-21T13:09:54.998+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:54.998+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-10-21T13:09:54.999+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:54 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.083016 s
[2024-10-21T13:09:55.003+0000] {spark_submit.py:634} INFO - +---------+---------+---+--------+-------+--------------------+------------+------------------+
[2024-10-21T13:09:55.003+0000] {spark_submit.py:634} INFO - |name_mark|    model| GB|  memory|  value|                link|data_collect|          category|
[2024-10-21T13:09:55.004+0000] {spark_submit.py:634} INFO - +---------+---------+---+--------+-------+--------------------+------------+------------------+
[2024-10-21T13:09:55.004+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 4060|  8|   GDDR6|1969.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.004+0000] {spark_submit.py:634} INFO - |   ASROCK|  RX 6600|  8|   GDDR6|1399.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.005+0000] {spark_submit.py:634} INFO - |   NVIDIA|  RTX4060|  8|   GDDR6|1959.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.005+0000] {spark_submit.py:634} INFO - |      AMD|RX 6650XT|  8|   GDDR6|1649.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.006+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 4060|  8|   GDDR6|2499.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.006+0000] {spark_submit.py:634} INFO - |      AMD|  RX 6600|  8|   GDDR6|1499.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.006+0000] {spark_submit.py:634} INFO - |      AMD|RX 6750XT| 12|   GDDR6|2249.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.007+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 4060|  8|   GDDR6|2139.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.007+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 3050|  6|   GDDR6|1119.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.007+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 3060| 12|   GDDR6|1899.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.007+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 3060| 12|   GDDR6|1779.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.008+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 3060| 12|   GDDR6|1639.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.008+0000] {spark_submit.py:634} INFO - |      AMD|  RX 7600|  8|   GDDR6|1799.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.008+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 4060|  8|   GDDR6|2199.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.009+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 4060|  8|   GDDR6|2441.41|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.009+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 4060|  0|   GDDR6|2649.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.010+0000] {spark_submit.py:634} INFO - |      AMD|  RX 6600|  8|   GDDR6|1459.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.010+0000] {spark_submit.py:634} INFO - |      AMD|  RX 6600|  8|   GDDR6|1399.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.010+0000] {spark_submit.py:634} INFO - |   NVIDIA| RTX 4070| 12|  GDDR6X|4199.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.011+0000] {spark_submit.py:634} INFO - |   ASROCK|   RX 570|  8|NOT_INFO| 799.99|https://www.kabum...|  2024-10-21|placa-de-video-vga|
[2024-10-21T13:09:55.011+0000] {spark_submit.py:634} INFO - +---------+---------+---+--------+-------+--------------------+------------+------------------+
[2024-10-21T13:09:55.012+0000] {spark_submit.py:634} INFO - only showing top 20 rows
[2024-10-21T13:09:55.012+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:55.525+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,placas-mae)
[2024-10-21T13:09:55.525+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = placas-mae)
[2024-10-21T13:09:55.560+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:55.573+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:55.574+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:55.575+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:55.575+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:55.576+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:55.576+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:55.642+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO BlockManagerInfo: Removed broadcast_9_piece0 on e2c3a206786d:37869 in memory (size: 11.0 KiB, free: 127.2 MiB)
[2024-10-21T13:09:55.666+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO CodeGenerator: Code generated in 17.449084 ms
[2024-10-21T13:09:55.669+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 199.7 KiB, free 126.8 MiB)
[2024-10-21T13:09:55.677+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.7 MiB)
[2024-10-21T13:09:55.679+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:55.680+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO SparkContext: Created broadcast 10 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:55.681+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:55.689+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:55.690+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:55.691+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:55.692+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:55.692+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:55.693+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:55.721+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 231.2 KiB, free 126.5 MiB)
[2024-10-21T13:09:55.723+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 82.1 KiB, free 126.4 MiB)
[2024-10-21T13:09:55.724+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on e2c3a206786d:37869 (size: 82.1 KiB, free: 127.1 MiB)
[2024-10-21T13:09:55.724+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:55.726+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:55.726+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-10-21T13:09:55.727+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:55.728+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2024-10-21T13:09:55.772+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO CodeGenerator: Code generated in 16.087315 ms
[2024-10-21T13:09:55.774+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:55.774+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:55.775+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:55.775+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:55.776+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:55.776+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:55.779+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:55.782+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:55.798+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-10-21T13:09:55.824+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-10-21T13:09:55.824+0000] {spark_submit.py:634} INFO - {
[2024-10-21T13:09:55.825+0000] {spark_submit.py:634} INFO - "type" : "struct",
[2024-10-21T13:09:55.825+0000] {spark_submit.py:634} INFO - "fields" : [ {
[2024-10-21T13:09:55.825+0000] {spark_submit.py:634} INFO - "name" : "name_mark",
[2024-10-21T13:09:55.826+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:55.827+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:55.827+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.827+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:55.828+0000] {spark_submit.py:634} INFO - "name" : "model",
[2024-10-21T13:09:55.828+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:55.828+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:55.829+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.829+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:55.830+0000] {spark_submit.py:634} INFO - "name" : "socket",
[2024-10-21T13:09:55.830+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:55.830+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:55.831+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.831+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:55.831+0000] {spark_submit.py:634} INFO - "name" : "DDR",
[2024-10-21T13:09:55.832+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:09:55.832+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:55.833+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.833+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:55.833+0000] {spark_submit.py:634} INFO - "name" : "value",
[2024-10-21T13:09:55.834+0000] {spark_submit.py:634} INFO - "type" : "decimal(12,2)",
[2024-10-21T13:09:55.834+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:55.834+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.835+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:55.835+0000] {spark_submit.py:634} INFO - "name" : "uri",
[2024-10-21T13:09:55.835+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:55.836+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:55.836+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.836+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:55.837+0000] {spark_submit.py:634} INFO - "name" : "data_collect",
[2024-10-21T13:09:55.837+0000] {spark_submit.py:634} INFO - "type" : "date",
[2024-10-21T13:09:55.837+0000] {spark_submit.py:634} INFO - "nullable" : false,
[2024-10-21T13:09:55.838+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.838+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:55.838+0000] {spark_submit.py:634} INFO - "name" : "category",
[2024-10-21T13:09:55.839+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:55.839+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:55.839+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:55.840+0000] {spark_submit.py:634} INFO - } ]
[2024-10-21T13:09:55.840+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:55.840+0000] {spark_submit.py:634} INFO - and corresponding Parquet message type:
[2024-10-21T13:09:55.841+0000] {spark_submit.py:634} INFO - message spark_schema {
[2024-10-21T13:09:55.841+0000] {spark_submit.py:634} INFO - optional binary name_mark (STRING);
[2024-10-21T13:09:55.842+0000] {spark_submit.py:634} INFO - optional binary model (STRING);
[2024-10-21T13:09:55.842+0000] {spark_submit.py:634} INFO - optional binary socket (STRING);
[2024-10-21T13:09:55.842+0000] {spark_submit.py:634} INFO - optional int64 DDR;
[2024-10-21T13:09:55.843+0000] {spark_submit.py:634} INFO - optional int64 value (DECIMAL(12,2));
[2024-10-21T13:09:55.843+0000] {spark_submit.py:634} INFO - optional binary uri (STRING);
[2024-10-21T13:09:55.843+0000] {spark_submit.py:634} INFO - required int32 data_collect (DATE);
[2024-10-21T13:09:55.844+0000] {spark_submit.py:634} INFO - optional binary category (STRING);
[2024-10-21T13:09:55.844+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:55.844+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:55.845+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:56.222+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO CodecPool: Got brand-new compressor [.snappy]
[2024-10-21T13:09:56.382+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:56.450+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO BlockManagerInfo: Removed broadcast_8_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:56.829+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO FileOutputCommitter: Saved output of task 'attempt_202410211309555114906974517182571_0005_m_000000_5' to file:/opt/***/datalake/silverzone/kabum_2024-10-21/placa_mae.parquet/_temporary/0/task_202410211309555114906974517182571_0005_m_000000
[2024-10-21T13:09:56.830+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO SparkHadoopMapRedUtil: attempt_202410211309555114906974517182571_0005_m_000000_5: Committed. Elapsed time: 26 ms.
[2024-10-21T13:09:56.835+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2669 bytes result sent to driver
[2024-10-21T13:09:56.837+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1110 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:56.837+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-10-21T13:09:56.838+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.145 s
[2024-10-21T13:09:56.838+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:56.839+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-10-21T13:09:56.839+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.148935 s
[2024-10-21T13:09:56.840+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:56 INFO FileFormatWriter: Start to commit write Job d9d25807-0a73-4edc-93ec-03d3d041ad70.
[2024-10-21T13:09:57.003+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileFormatWriter: Write Job d9d25807-0a73-4edc-93ec-03d3d041ad70 committed. Elapsed time: 161 ms.
[2024-10-21T13:09:57.005+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileFormatWriter: Finished processing stats for write job d9d25807-0a73-4edc-93ec-03d3d041ad70.
[2024-10-21T13:09:57.039+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,memoria-ram)
[2024-10-21T13:09:57.040+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = memoria-ram)
[2024-10-21T13:09:57.050+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:57.051+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:57.052+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:57.052+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:57.053+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:57.053+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:57.054+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:57.117+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on e2c3a206786d:37869 in memory (size: 82.1 KiB, free: 127.2 MiB)
[2024-10-21T13:09:57.150+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO CodeGenerator: Code generated in 31.891615 ms
[2024-10-21T13:09:57.154+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 199.7 KiB, free 126.8 MiB)
[2024-10-21T13:09:57.161+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.7 MiB)
[2024-10-21T13:09:57.162+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:57.162+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SparkContext: Created broadcast 12 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:57.164+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:57.169+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:57.170+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:57.170+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:57.171+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:57.171+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:57.171+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[32] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:57.189+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 239.1 KiB, free 126.5 MiB)
[2024-10-21T13:09:57.191+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 84.1 KiB, free 126.4 MiB)
[2024-10-21T13:09:57.192+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on e2c3a206786d:37869 (size: 84.1 KiB, free: 127.1 MiB)
[2024-10-21T13:09:57.193+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:57.193+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[32] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:57.194+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-10-21T13:09:57.195+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:57.201+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2024-10-21T13:09:57.231+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO CodeGenerator: Code generated in 22.11492 ms
[2024-10-21T13:09:57.233+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:57.234+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:57.234+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:57.234+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:57.235+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:57.235+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:57.235+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:57.236+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:57.236+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-10-21T13:09:57.237+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-10-21T13:09:57.237+0000] {spark_submit.py:634} INFO - {
[2024-10-21T13:09:57.238+0000] {spark_submit.py:634} INFO - "type" : "struct",
[2024-10-21T13:09:57.238+0000] {spark_submit.py:634} INFO - "fields" : [ {
[2024-10-21T13:09:57.238+0000] {spark_submit.py:634} INFO - "name" : "memory_name",
[2024-10-21T13:09:57.239+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:57.239+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.240+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.240+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.240+0000] {spark_submit.py:634} INFO - "name" : "name_mark",
[2024-10-21T13:09:57.241+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:57.241+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.242+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.242+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.242+0000] {spark_submit.py:634} INFO - "name" : "MHZ",
[2024-10-21T13:09:57.243+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:09:57.243+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.244+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.244+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.245+0000] {spark_submit.py:634} INFO - "name" : "GB",
[2024-10-21T13:09:57.245+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:09:57.245+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.246+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.246+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.247+0000] {spark_submit.py:634} INFO - "name" : "DDR",
[2024-10-21T13:09:57.247+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:09:57.247+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.248+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.248+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.248+0000] {spark_submit.py:634} INFO - "name" : "latency",
[2024-10-21T13:09:57.249+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:09:57.249+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.250+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.250+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.251+0000] {spark_submit.py:634} INFO - "name" : "value",
[2024-10-21T13:09:57.251+0000] {spark_submit.py:634} INFO - "type" : "decimal(12,2)",
[2024-10-21T13:09:57.252+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.252+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.252+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.253+0000] {spark_submit.py:634} INFO - "name" : "uri",
[2024-10-21T13:09:57.253+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:57.254+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.254+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.254+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.255+0000] {spark_submit.py:634} INFO - "name" : "data_collect",
[2024-10-21T13:09:57.255+0000] {spark_submit.py:634} INFO - "type" : "date",
[2024-10-21T13:09:57.255+0000] {spark_submit.py:634} INFO - "nullable" : false,
[2024-10-21T13:09:57.256+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.256+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:57.256+0000] {spark_submit.py:634} INFO - "name" : "category",
[2024-10-21T13:09:57.257+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:57.257+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:57.257+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:57.258+0000] {spark_submit.py:634} INFO - } ]
[2024-10-21T13:09:57.258+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:57.258+0000] {spark_submit.py:634} INFO - and corresponding Parquet message type:
[2024-10-21T13:09:57.259+0000] {spark_submit.py:634} INFO - message spark_schema {
[2024-10-21T13:09:57.259+0000] {spark_submit.py:634} INFO - optional binary memory_name (STRING);
[2024-10-21T13:09:57.259+0000] {spark_submit.py:634} INFO - optional binary name_mark (STRING);
[2024-10-21T13:09:57.259+0000] {spark_submit.py:634} INFO - optional int64 MHZ;
[2024-10-21T13:09:57.260+0000] {spark_submit.py:634} INFO - optional int64 GB;
[2024-10-21T13:09:57.260+0000] {spark_submit.py:634} INFO - optional int64 DDR;
[2024-10-21T13:09:57.260+0000] {spark_submit.py:634} INFO - optional int64 latency;
[2024-10-21T13:09:57.261+0000] {spark_submit.py:634} INFO - optional int64 value (DECIMAL(12,2));
[2024-10-21T13:09:57.261+0000] {spark_submit.py:634} INFO - optional binary uri (STRING);
[2024-10-21T13:09:57.261+0000] {spark_submit.py:634} INFO - required int32 data_collect (DATE);
[2024-10-21T13:09:57.262+0000] {spark_submit.py:634} INFO - optional binary category (STRING);
[2024-10-21T13:09:57.262+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:57.262+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:57.263+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:57.319+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:57.444+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileOutputCommitter: Saved output of task 'attempt_202410211309577626220579725894872_0006_m_000000_6' to file:/opt/***/datalake/silverzone/kabum_2024-10-21/memoria_k.parquet/_temporary/0/task_202410211309577626220579725894872_0006_m_000000
[2024-10-21T13:09:57.444+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO SparkHadoopMapRedUtil: attempt_202410211309577626220579725894872_0006_m_000000_6: Committed. Elapsed time: 30 ms.
[2024-10-21T13:09:57.446+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2583 bytes result sent to driver
[2024-10-21T13:09:57.447+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 252 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:57.447+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-10-21T13:09:57.448+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.276 s
[2024-10-21T13:09:57.449+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:57.449+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-10-21T13:09:57.450+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO DAGScheduler: Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.278885 s
[2024-10-21T13:09:57.450+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:57 INFO FileFormatWriter: Start to commit write Job e310f354-1075-4d54-89c5-25934eb4481e.
[2024-10-21T13:09:58.004+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileFormatWriter: Write Job e310f354-1075-4d54-89c5-25934eb4481e committed. Elapsed time: 555 ms.
[2024-10-21T13:09:58.005+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileFormatWriter: Finished processing stats for write job e310f354-1075-4d54-89c5-25934eb4481e.
[2024-10-21T13:09:58.035+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO BlockManagerInfo: Removed broadcast_13_piece0 on e2c3a206786d:37869 in memory (size: 84.1 KiB, free: 127.1 MiB)
[2024-10-21T13:09:58.054+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,processadores)
[2024-10-21T13:09:58.054+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = processadores)
[2024-10-21T13:09:58.066+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:58.068+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:58.086+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:58.086+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:58.087+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:58.088+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:58.089+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:58.165+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO CodeGenerator: Code generated in 27.548295 ms
[2024-10-21T13:09:58.168+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 199.7 KiB, free 126.5 MiB)
[2024-10-21T13:09:58.177+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.5 MiB)
[2024-10-21T13:09:58.179+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.1 MiB)
[2024-10-21T13:09:58.180+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SparkContext: Created broadcast 14 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:58.181+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:58.196+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:58.198+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Got job 7 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:58.200+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:58.201+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:58.202+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:58.202+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[36] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:58.222+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 233.3 KiB, free 126.3 MiB)
[2024-10-21T13:09:58.224+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 82.3 KiB, free 126.2 MiB)
[2024-10-21T13:09:58.225+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on e2c3a206786d:37869 (size: 82.3 KiB, free: 127.0 MiB)
[2024-10-21T13:09:58.227+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:58.230+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[36] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:58.231+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-10-21T13:09:58.232+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:58.233+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2024-10-21T13:09:58.282+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO CodeGenerator: Code generated in 32.515447 ms
[2024-10-21T13:09:58.285+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:58.285+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:58.286+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:58.286+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:58.287+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:58.287+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:58.288+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:58.288+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:58.289+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-10-21T13:09:58.289+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-10-21T13:09:58.290+0000] {spark_submit.py:634} INFO - {
[2024-10-21T13:09:58.291+0000] {spark_submit.py:634} INFO - "type" : "struct",
[2024-10-21T13:09:58.292+0000] {spark_submit.py:634} INFO - "fields" : [ {
[2024-10-21T13:09:58.292+0000] {spark_submit.py:634} INFO - "name" : "name_mark",
[2024-10-21T13:09:58.295+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:58.295+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:58.296+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.297+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:58.298+0000] {spark_submit.py:634} INFO - "name" : "model",
[2024-10-21T13:09:58.299+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:58.300+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:58.302+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.302+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:58.303+0000] {spark_submit.py:634} INFO - "name" : "frequency_base",
[2024-10-21T13:09:58.303+0000] {spark_submit.py:634} INFO - "type" : "double",
[2024-10-21T13:09:58.303+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:58.304+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.305+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:58.305+0000] {spark_submit.py:634} INFO - "name" : "cache",
[2024-10-21T13:09:58.305+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:09:58.306+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:58.306+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.306+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:58.307+0000] {spark_submit.py:634} INFO - "name" : "value",
[2024-10-21T13:09:58.308+0000] {spark_submit.py:634} INFO - "type" : "decimal(12,2)",
[2024-10-21T13:09:58.308+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:58.309+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.309+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:58.311+0000] {spark_submit.py:634} INFO - "name" : "uri",
[2024-10-21T13:09:58.312+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:58.312+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:58.315+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.316+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:58.316+0000] {spark_submit.py:634} INFO - "name" : "data_collect",
[2024-10-21T13:09:58.317+0000] {spark_submit.py:634} INFO - "type" : "date",
[2024-10-21T13:09:58.317+0000] {spark_submit.py:634} INFO - "nullable" : false,
[2024-10-21T13:09:58.318+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.318+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:58.321+0000] {spark_submit.py:634} INFO - "name" : "category",
[2024-10-21T13:09:58.321+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:58.322+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:58.322+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:58.323+0000] {spark_submit.py:634} INFO - } ]
[2024-10-21T13:09:58.323+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:58.323+0000] {spark_submit.py:634} INFO - and corresponding Parquet message type:
[2024-10-21T13:09:58.324+0000] {spark_submit.py:634} INFO - message spark_schema {
[2024-10-21T13:09:58.324+0000] {spark_submit.py:634} INFO - optional binary name_mark (STRING);
[2024-10-21T13:09:58.324+0000] {spark_submit.py:634} INFO - optional binary model (STRING);
[2024-10-21T13:09:58.325+0000] {spark_submit.py:634} INFO - optional double frequency_base;
[2024-10-21T13:09:58.326+0000] {spark_submit.py:634} INFO - optional int64 cache;
[2024-10-21T13:09:58.326+0000] {spark_submit.py:634} INFO - optional int64 value (DECIMAL(12,2));
[2024-10-21T13:09:58.327+0000] {spark_submit.py:634} INFO - optional binary uri (STRING);
[2024-10-21T13:09:58.331+0000] {spark_submit.py:634} INFO - required int32 data_collect (DATE);
[2024-10-21T13:09:58.331+0000] {spark_submit.py:634} INFO - optional binary category (STRING);
[2024-10-21T13:09:58.332+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:58.333+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:58.333+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:58.580+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:58.877+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileOutputCommitter: Saved output of task 'attempt_20241021130958284958112079246413_0007_m_000000_7' to file:/opt/***/datalake/silverzone/kabum_2024-10-21/processador_k.parquet/_temporary/0/task_20241021130958284958112079246413_0007_m_000000
[2024-10-21T13:09:58.879+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO SparkHadoopMapRedUtil: attempt_20241021130958284958112079246413_0007_m_000000_7: Committed. Elapsed time: 33 ms.
[2024-10-21T13:09:58.880+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2583 bytes result sent to driver
[2024-10-21T13:09:58.880+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 650 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:58.881+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-10-21T13:09:58.882+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.677 s
[2024-10-21T13:09:58.882+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:58.883+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-10-21T13:09:58.883+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO DAGScheduler: Job 7 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.685260 s
[2024-10-21T13:09:58.884+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:58 INFO FileFormatWriter: Start to commit write Job ba1548f1-6d83-4a45-8e8e-46efaec69086.
[2024-10-21T13:09:59.101+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileFormatWriter: Write Job ba1548f1-6d83-4a45-8e8e-46efaec69086 committed. Elapsed time: 218 ms.
[2024-10-21T13:09:59.101+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileFormatWriter: Finished processing stats for write job ba1548f1-6d83-4a45-8e8e-46efaec69086.
[2024-10-21T13:09:59.129+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,placa-de-video-vga)
[2024-10-21T13:09:59.130+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = placa-de-video-vga)
[2024-10-21T13:09:59.139+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:59.140+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:59.141+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:59.141+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:59.142+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:59.142+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:59.143+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:59.233+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO CodeGenerator: Code generated in 21.270977 ms
[2024-10-21T13:09:59.237+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 199.7 KiB, free 126.0 MiB)
[2024-10-21T13:09:59.251+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.0 MiB)
[2024-10-21T13:09:59.253+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.0 MiB)
[2024-10-21T13:09:59.253+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SparkContext: Created broadcast 16 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:59.254+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:09:59.270+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:09:59.271+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Got job 8 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:09:59.273+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:09:59.275+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:09:59.276+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:09:59.276+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:09:59.290+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 231.4 KiB, free 125.8 MiB)
[2024-10-21T13:09:59.292+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 82.2 KiB, free 125.7 MiB)
[2024-10-21T13:09:59.292+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on e2c3a206786d:37869 (size: 82.2 KiB, free: 126.9 MiB)
[2024-10-21T13:09:59.293+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:09:59.294+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:09:59.294+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-10-21T13:09:59.295+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9625 bytes)
[2024-10-21T13:09:59.297+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2024-10-21T13:09:59.337+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO CodeGenerator: Code generated in 24.492541 ms
[2024-10-21T13:09:59.339+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:59.339+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:59.340+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:59.340+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:59.341+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:59.341+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:59.342+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:59.342+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:09:59.343+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-10-21T13:09:59.343+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-10-21T13:09:59.344+0000] {spark_submit.py:634} INFO - {
[2024-10-21T13:09:59.345+0000] {spark_submit.py:634} INFO - "type" : "struct",
[2024-10-21T13:09:59.345+0000] {spark_submit.py:634} INFO - "fields" : [ {
[2024-10-21T13:09:59.346+0000] {spark_submit.py:634} INFO - "name" : "name_mark",
[2024-10-21T13:09:59.346+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:59.347+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:59.347+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.347+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:59.348+0000] {spark_submit.py:634} INFO - "name" : "model",
[2024-10-21T13:09:59.350+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:59.350+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:59.351+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.351+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:59.352+0000] {spark_submit.py:634} INFO - "name" : "GB",
[2024-10-21T13:09:59.353+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:09:59.354+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:59.354+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.355+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:59.355+0000] {spark_submit.py:634} INFO - "name" : "memory",
[2024-10-21T13:09:59.356+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:59.356+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:59.356+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.357+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:59.357+0000] {spark_submit.py:634} INFO - "name" : "value",
[2024-10-21T13:09:59.358+0000] {spark_submit.py:634} INFO - "type" : "decimal(12,2)",
[2024-10-21T13:09:59.358+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:59.358+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.359+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:59.359+0000] {spark_submit.py:634} INFO - "name" : "link",
[2024-10-21T13:09:59.360+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:59.361+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:59.361+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.362+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:59.362+0000] {spark_submit.py:634} INFO - "name" : "data_collect",
[2024-10-21T13:09:59.362+0000] {spark_submit.py:634} INFO - "type" : "date",
[2024-10-21T13:09:59.363+0000] {spark_submit.py:634} INFO - "nullable" : false,
[2024-10-21T13:09:59.363+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.364+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:09:59.364+0000] {spark_submit.py:634} INFO - "name" : "category",
[2024-10-21T13:09:59.366+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:09:59.367+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:09:59.367+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:09:59.368+0000] {spark_submit.py:634} INFO - } ]
[2024-10-21T13:09:59.368+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:59.369+0000] {spark_submit.py:634} INFO - and corresponding Parquet message type:
[2024-10-21T13:09:59.369+0000] {spark_submit.py:634} INFO - message spark_schema {
[2024-10-21T13:09:59.370+0000] {spark_submit.py:634} INFO - optional binary name_mark (STRING);
[2024-10-21T13:09:59.370+0000] {spark_submit.py:634} INFO - optional binary model (STRING);
[2024-10-21T13:09:59.370+0000] {spark_submit.py:634} INFO - optional int64 GB;
[2024-10-21T13:09:59.371+0000] {spark_submit.py:634} INFO - optional binary memory (STRING);
[2024-10-21T13:09:59.371+0000] {spark_submit.py:634} INFO - optional int64 value (DECIMAL(12,2));
[2024-10-21T13:09:59.371+0000] {spark_submit.py:634} INFO - optional binary link (STRING);
[2024-10-21T13:09:59.372+0000] {spark_submit.py:634} INFO - required int32 data_collect (DATE);
[2024-10-21T13:09:59.372+0000] {spark_submit.py:634} INFO - optional binary category (STRING);
[2024-10-21T13:09:59.372+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:09:59.373+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:59.373+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:09:59.605+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:09:59.746+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: Saved output of task 'attempt_202410211309592545072351945965247_0008_m_000000_8' to file:/opt/***/datalake/silverzone/kabum_2024-10-21/placa_video_k.parquet/_temporary/0/task_202410211309592545072351945965247_0008_m_000000
[2024-10-21T13:09:59.747+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SparkHadoopMapRedUtil: attempt_202410211309592545072351945965247_0008_m_000000_8: Committed. Elapsed time: 32 ms.
[2024-10-21T13:09:59.749+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2583 bytes result sent to driver
[2024-10-21T13:09:59.750+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 455 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:09:59.750+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-10-21T13:09:59.751+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.478 s
[2024-10-21T13:09:59.752+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:09:59.752+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-10-21T13:09:59.753+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO DAGScheduler: Job 8 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.481475 s
[2024-10-21T13:09:59.753+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileFormatWriter: Start to commit write Job 586085e8-fa68-48e1-8bf6-cbad00a9f849.
[2024-10-21T13:09:59.948+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileFormatWriter: Write Job 586085e8-fa68-48e1-8bf6-cbad00a9f849 committed. Elapsed time: 196 ms.
[2024-10-21T13:09:59.951+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileFormatWriter: Finished processing stats for write job 586085e8-fa68-48e1-8bf6-cbad00a9f849.
[2024-10-21T13:09:59.968+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:59.969+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:59.969+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:59.970+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:09:59.970+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:09:59.971+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:09:59.971+0000] {spark_submit.py:634} INFO - 24/10/21 13:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:00.025+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO CodeGenerator: Code generated in 6.271617 ms
[2024-10-21T13:10:00.034+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:00.036+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO DAGScheduler: Got job 9 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:00.038+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:00.038+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:10:00.039+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:00.041+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[42] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:00.069+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 214.0 KiB, free 125.5 MiB)
[2024-10-21T13:10:00.072+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 77.8 KiB, free 125.4 MiB)
[2024-10-21T13:10:00.073+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on e2c3a206786d:37869 (size: 77.8 KiB, free: 126.8 MiB)
[2024-10-21T13:10:00.074+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:00.075+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[42] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:00.076+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-10-21T13:10:00.080+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9030 bytes)
[2024-10-21T13:10:00.081+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:00 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2024-10-21T13:10:01.190+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO CodeGenerator: Code generated in 5.479177 ms
[2024-10-21T13:10:01.191+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:01.191+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:01.192+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:01.192+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:01.193+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:01.193+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:01.194+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:10:01.194+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:10:01.194+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-10-21T13:10:01.195+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-10-21T13:10:01.195+0000] {spark_submit.py:634} INFO - {
[2024-10-21T13:10:01.196+0000] {spark_submit.py:634} INFO - "type" : "struct",
[2024-10-21T13:10:01.196+0000] {spark_submit.py:634} INFO - "fields" : [ {
[2024-10-21T13:10:01.197+0000] {spark_submit.py:634} INFO - "name" : "site",
[2024-10-21T13:10:01.197+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:10:01.198+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:10:01.198+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:10:01.198+0000] {spark_submit.py:634} INFO - }, {
[2024-10-21T13:10:01.199+0000] {spark_submit.py:634} INFO - "name" : "id_site",
[2024-10-21T13:10:01.199+0000] {spark_submit.py:634} INFO - "type" : "long",
[2024-10-21T13:10:01.199+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:10:01.200+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:10:01.200+0000] {spark_submit.py:634} INFO - } ]
[2024-10-21T13:10:01.200+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:10:01.201+0000] {spark_submit.py:634} INFO - and corresponding Parquet message type:
[2024-10-21T13:10:01.201+0000] {spark_submit.py:634} INFO - message spark_schema {
[2024-10-21T13:10:01.202+0000] {spark_submit.py:634} INFO - optional binary site (STRING);
[2024-10-21T13:10:01.202+0000] {spark_submit.py:634} INFO - optional int64 id_site;
[2024-10-21T13:10:01.202+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:10:01.203+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:10:01.203+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:10:01.372+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO PythonRunner: Times: total = 1226, boot = 1036, init = 190, finish = 0
[2024-10-21T13:10:01.448+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileOutputCommitter: Saved output of task 'attempt_202410211310007731866410499403995_0009_m_000000_9' to file:/opt/***/datalake/silverzone/kabum_2024-10-21/dim_site.parquet/_temporary/0/task_202410211310007731866410499403995_0009_m_000000
[2024-10-21T13:10:01.449+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO SparkHadoopMapRedUtil: attempt_202410211310007731866410499403995_0009_m_000000_9: Committed. Elapsed time: 34 ms.
[2024-10-21T13:10:01.450+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2875 bytes result sent to driver
[2024-10-21T13:10:01.452+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1375 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:01.453+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-10-21T13:10:01.453+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33071
[2024-10-21T13:10:01.454+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO DAGScheduler: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.416 s
[2024-10-21T13:10:01.455+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:10:01.456+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-10-21T13:10:01.456+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO DAGScheduler: Job 9 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.420655 s
[2024-10-21T13:10:01.456+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileFormatWriter: Start to commit write Job 3beefaa8-b3cb-4a4e-a773-321f645d0ab2.
[2024-10-21T13:10:01.645+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileFormatWriter: Write Job 3beefaa8-b3cb-4a4e-a773-321f645d0ab2 committed. Elapsed time: 188 ms.
[2024-10-21T13:10:01.645+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileFormatWriter: Finished processing stats for write job 3beefaa8-b3cb-4a4e-a773-321f645d0ab2.
[2024-10-21T13:10:01.759+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,placa-de-video-vga)
[2024-10-21T13:10:01.760+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#8),(category#8 = placa-de-video-vga)
[2024-10-21T13:10:01.761+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,processadores)
[2024-10-21T13:10:01.762+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#677),(category#677 = processadores)
[2024-10-21T13:10:01.763+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,placas-mae)
[2024-10-21T13:10:01.763+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#683),(category#683 = placas-mae)
[2024-10-21T13:10:01.765+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category),EqualTo(category,memoria-ram)
[2024-10-21T13:10:01.765+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category#689),(category#689 = memoria-ram)
[2024-10-21T13:10:01.969+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO CodeGenerator: Code generated in 62.822682 ms
[2024-10-21T13:10:01.973+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 199.7 KiB, free 125.2 MiB)
[2024-10-21T13:10:01.992+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 125.2 MiB)
[2024-10-21T13:10:01.994+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 126.8 MiB)
[2024-10-21T13:10:01.995+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO SparkContext: Created broadcast 19 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:01.996+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:10:02.034+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_18_piece0 on e2c3a206786d:37869 in memory (size: 77.8 KiB, free: 126.9 MiB)
[2024-10-21T13:10:02.048+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_16_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 126.9 MiB)
[2024-10-21T13:10:02.054+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on e2c3a206786d:37869 in memory (size: 82.3 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.065+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_14_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.074+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_17_piece0 on e2c3a206786d:37869 in memory (size: 82.2 KiB, free: 127.1 MiB)
[2024-10-21T13:10:02.097+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Registering RDD 46 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-10-21T13:10:02.098+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Got map stage job 10 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:02.106+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:02.110+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:10:02.112+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:02.118+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[46] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:02.123+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 42.2 KiB, free 126.5 MiB)
[2024-10-21T13:10:02.137+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 126.5 MiB)
[2024-10-21T13:10:02.138+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 37.994524 ms
[2024-10-21T13:10:02.139+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on e2c3a206786d:37869 (size: 19.5 KiB, free: 127.1 MiB)
[2024-10-21T13:10:02.139+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:02.142+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 199.7 KiB, free 126.3 MiB)
[2024-10-21T13:10:02.142+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[46] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:02.143+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-10-21T13:10:02.146+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9614 bytes)
[2024-10-21T13:10:02.147+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2024-10-21T13:10:02.151+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 126.2 MiB)
[2024-10-21T13:10:02.155+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.157+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO SparkContext: Created broadcast 21 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:02.159+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:10:02.165+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Registering RDD 50 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2024-10-21T13:10:02.169+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Got map stage job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:02.170+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:02.172+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:10:02.172+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:02.173+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[50] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:02.175+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 42.1 KiB, free 126.2 MiB)
[2024-10-21T13:10:02.181+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 126.2 MiB)
[2024-10-21T13:10:02.182+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on e2c3a206786d:37869 (size: 19.4 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.184+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:02.186+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[50] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:02.186+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-10-21T13:10:02.191+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 28.596049 ms
[2024-10-21T13:10:02.200+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 25.935914 ms
[2024-10-21T13:10:02.204+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 199.7 KiB, free 126.0 MiB)
[2024-10-21T13:10:02.206+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 9.632887 ms
[2024-10-21T13:10:02.217+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 125.9 MiB)
[2024-10-21T13:10:02.219+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.221+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO SparkContext: Created broadcast 23 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:02.222+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:10:02.228+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Registering RDD 54 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 2
[2024-10-21T13:10:02.230+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 7.10426 ms
[2024-10-21T13:10:02.251+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Got map stage job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:02.252+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:02.254+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:10:02.258+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:02.259+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:02.259+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 42.2 KiB, free 121.9 MiB)
[2024-10-21T13:10:02.265+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 121.6 MiB)
[2024-10-21T13:10:02.266+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on e2c3a206786d:37869 (size: 19.5 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.267+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:02.269+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:02.270+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-10-21T13:10:02.270+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:10:02.274+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 4.216814 ms
[2024-10-21T13:10:02.298+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 32.112826 ms
[2024-10-21T13:10:02.305+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 199.7 KiB, free 121.4 MiB)
[2024-10-21T13:10:02.312+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 121.4 MiB)
[2024-10-21T13:10:02.313+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 126.9 MiB)
[2024-10-21T13:10:02.314+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO SparkContext: Created broadcast 25 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:02.314+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:10:02.324+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Registering RDD 58 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 3
[2024-10-21T13:10:02.332+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Got map stage job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:02.333+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:02.334+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:10:02.337+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:02.337+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[58] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:02.339+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 42.3 KiB, free 121.4 MiB)
[2024-10-21T13:10:02.340+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 121.3 MiB)
[2024-10-21T13:10:02.340+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on e2c3a206786d:37869 (size: 19.6 KiB, free: 126.9 MiB)
[2024-10-21T13:10:02.342+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:02.343+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[58] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:02.344+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-10-21T13:10:02.441+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2864 bytes result sent to driver
[2024-10-21T13:10:02.464+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9614 bytes)
[2024-10-21T13:10:02.524+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2024-10-21T13:10:02.547+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 299 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:02.567+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-10-21T13:10:02.573+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: ShuffleMapStage 10 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.354 s
[2024-10-21T13:10:02.584+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: looking for newly runnable stages
[2024-10-21T13:10:02.600+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: running: Set(ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 11)
[2024-10-21T13:10:02.617+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: waiting: Set()
[2024-10-21T13:10:02.626+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: failed: Set()
[2024-10-21T13:10:02.629+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 65.265405 ms
[2024-10-21T13:10:02.633+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:10:02.712+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2778 bytes result sent to driver
[2024-10-21T13:10:02.714+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9614 bytes)
[2024-10-21T13:10:02.717+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 272 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:02.720+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2024-10-21T13:10:02.721+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-10-21T13:10:02.722+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: ShuffleMapStage 11 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.543 s
[2024-10-21T13:10:02.722+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: looking for newly runnable stages
[2024-10-21T13:10:02.723+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: running: Set(ShuffleMapStage 12, ShuffleMapStage 13)
[2024-10-21T13:10:02.723+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: waiting: Set()
[2024-10-21T13:10:02.724+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: failed: Set()
[2024-10-21T13:10:02.744+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 24.540643 ms
[2024-10-21T13:10:02.754+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:10:02.811+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2778 bytes result sent to driver
[2024-10-21T13:10:02.814+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9614 bytes)
[2024-10-21T13:10:02.816+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2024-10-21T13:10:02.817+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 101 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:02.818+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-10-21T13:10:02.819+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: ShuffleMapStage 12 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.578 s
[2024-10-21T13:10:02.820+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: looking for newly runnable stages
[2024-10-21T13:10:02.821+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: running: Set(ShuffleMapStage 13)
[2024-10-21T13:10:02.822+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: waiting: Set()
[2024-10-21T13:10:02.822+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: failed: Set()
[2024-10-21T13:10:02.843+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 21.002064 ms
[2024-10-21T13:10:02.865+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:10:02.906+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_20_piece0 on e2c3a206786d:37869 in memory (size: 19.5 KiB, free: 126.9 MiB)
[2024-10-21T13:10:02.919+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_24_piece0 on e2c3a206786d:37869 in memory (size: 19.5 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.924+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO BlockManagerInfo: Removed broadcast_22_piece0 on e2c3a206786d:37869 in memory (size: 19.4 KiB, free: 127.0 MiB)
[2024-10-21T13:10:02.939+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2821 bytes result sent to driver
[2024-10-21T13:10:02.940+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 128 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:02.941+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-10-21T13:10:02.942+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.616 s
[2024-10-21T13:10:02.942+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: looking for newly runnable stages
[2024-10-21T13:10:02.942+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: running: Set()
[2024-10-21T13:10:02.943+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: waiting: Set()
[2024-10-21T13:10:02.943+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO DAGScheduler: failed: Set()
[2024-10-21T13:10:02.953+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-10-21T13:10:02.957+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-10-21T13:10:02.958+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-10-21T13:10:02.959+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-10-21T13:10:02.998+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:02 INFO CodeGenerator: Code generated in 15.609591 ms
[2024-10-21T13:10:03.017+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 15.275874 ms
[2024-10-21T13:10:03.052+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 16.097216 ms
[2024-10-21T13:10:03.074+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 14.909655 ms
[2024-10-21T13:10:03.094+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 13.572987 ms
[2024-10-21T13:10:03.156+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Registering RDD 69 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 4
[2024-10-21T13:10:03.159+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Got map stage job 14 (parquet at NativeMethodAccessorImpl.java:0) with 4 output partitions
[2024-10-21T13:10:03.159+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:03.160+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 14)
[2024-10-21T13:10:03.160+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:03.161+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[69] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:03.176+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 114.9 KiB, free 125.7 MiB)
[2024-10-21T13:10:03.178+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 44.7 KiB, free 125.6 MiB)
[2024-10-21T13:10:03.178+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on e2c3a206786d:37869 (size: 44.7 KiB, free: 126.9 MiB)
[2024-10-21T13:10:03.179+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:03.180+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[69] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2024-10-21T13:10:03.180+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSchedulerImpl: Adding task set 18.0 with 4 tasks resource profile 0
[2024-10-21T13:10:03.185+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (e2c3a206786d, executor driver, partition 0, NODE_LOCAL, 9097 bytes)
[2024-10-21T13:10:03.186+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Running task 0.0 in stage 18.0 (TID 14)
[2024-10-21T13:10:03.216+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Getting 1 (792.0 B) non-empty blocks including 1 (792.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-10-21T13:10:03.218+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2024-10-21T13:10:03.233+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 12.571936 ms
[2024-10-21T13:10:03.246+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 11.817699 ms
[2024-10-21T13:10:03.271+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Finished task 0.0 in stage 18.0 (TID 14). 12797 bytes result sent to driver
[2024-10-21T13:10:03.271+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 15) (e2c3a206786d, executor driver, partition 1, NODE_LOCAL, 9097 bytes)
[2024-10-21T13:10:03.272+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 89 ms on e2c3a206786d (executor driver) (1/4)
[2024-10-21T13:10:03.273+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Running task 1.0 in stage 18.0 (TID 15)
[2024-10-21T13:10:03.281+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Getting 1 (21.2 KiB) non-empty blocks including 1 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-10-21T13:10:03.282+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-10-21T13:10:03.294+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 12.859751 ms
[2024-10-21T13:10:03.338+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Finished task 1.0 in stage 18.0 (TID 15). 12797 bytes result sent to driver
[2024-10-21T13:10:03.339+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 16) (e2c3a206786d, executor driver, partition 2, NODE_LOCAL, 9097 bytes)
[2024-10-21T13:10:03.339+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Running task 2.0 in stage 18.0 (TID 16)
[2024-10-21T13:10:03.340+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 15) in 68 ms on e2c3a206786d (executor driver) (2/4)
[2024-10-21T13:10:03.347+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Getting 1 (1329.0 B) non-empty blocks including 1 (1329.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-10-21T13:10:03.347+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-10-21T13:10:03.361+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 13.331975 ms
[2024-10-21T13:10:03.374+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on e2c3a206786d:37869 in memory (size: 19.6 KiB, free: 127.0 MiB)
[2024-10-21T13:10:03.385+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Finished task 2.0 in stage 18.0 (TID 16). 12840 bytes result sent to driver
[2024-10-21T13:10:03.386+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Starting task 3.0 in stage 18.0 (TID 17) (e2c3a206786d, executor driver, partition 3, NODE_LOCAL, 9097 bytes)
[2024-10-21T13:10:03.386+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Running task 3.0 in stage 18.0 (TID 17)
[2024-10-21T13:10:03.387+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 16) in 49 ms on e2c3a206786d (executor driver) (3/4)
[2024-10-21T13:10:03.394+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Getting 1 (1705.0 B) non-empty blocks including 1 (1705.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-10-21T13:10:03.394+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-10-21T13:10:03.406+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 11.66109 ms
[2024-10-21T13:10:03.422+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Finished task 3.0 in stage 18.0 (TID 17). 12797 bytes result sent to driver
[2024-10-21T13:10:03.423+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Finished task 3.0 in stage 18.0 (TID 17) in 38 ms on e2c3a206786d (executor driver) (4/4)
[2024-10-21T13:10:03.424+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-10-21T13:10:03.425+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: ShuffleMapStage 18 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.252 s
[2024-10-21T13:10:03.428+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: looking for newly runnable stages
[2024-10-21T13:10:03.428+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: running: Set()
[2024-10-21T13:10:03.429+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: waiting: Set()
[2024-10-21T13:10:03.429+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: failed: Set()
[2024-10-21T13:10:03.430+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-10-21T13:10:03.439+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:03.440+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:03.441+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:03.441+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:03.442+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:03.442+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:03.442+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:03.500+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 11.465581 ms
[2024-10-21T13:10:03.507+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:03.508+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Got job 15 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:03.509+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Final stage: ResultStage 24 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:03.509+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
[2024-10-21T13:10:03.510+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:03.510+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[72] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:03.524+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 280.5 KiB, free 125.4 MiB)
[2024-10-21T13:10:03.526+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 100.8 KiB, free 125.3 MiB)
[2024-10-21T13:10:03.527+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on e2c3a206786d:37869 (size: 100.8 KiB, free: 126.9 MiB)
[2024-10-21T13:10:03.528+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:03.528+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[72] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:03.529+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2024-10-21T13:10:03.529+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18) (e2c3a206786d, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2024-10-21T13:10:03.530+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Running task 0.0 in stage 24.0 (TID 18)
[2024-10-21T13:10:03.543+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Getting 4 (24.9 KiB) non-empty blocks including 4 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-10-21T13:10:03.543+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-10-21T13:10:03.555+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 11.925704 ms
[2024-10-21T13:10:03.556+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:03.557+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:03.557+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:03.558+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:03.558+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:03.558+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:03.559+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:10:03.559+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:10:03.560+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-10-21T13:10:03.560+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-10-21T13:10:03.560+0000] {spark_submit.py:634} INFO - {
[2024-10-21T13:10:03.561+0000] {spark_submit.py:634} INFO - "type" : "struct",
[2024-10-21T13:10:03.561+0000] {spark_submit.py:634} INFO - "fields" : [ {
[2024-10-21T13:10:03.562+0000] {spark_submit.py:634} INFO - "name" : "name_mark",
[2024-10-21T13:10:03.562+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:10:03.563+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:10:03.563+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:10:03.563+0000] {spark_submit.py:634} INFO - } ]
[2024-10-21T13:10:03.564+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:10:03.564+0000] {spark_submit.py:634} INFO - and corresponding Parquet message type:
[2024-10-21T13:10:03.564+0000] {spark_submit.py:634} INFO - message spark_schema {
[2024-10-21T13:10:03.565+0000] {spark_submit.py:634} INFO - optional binary name_mark (STRING);
[2024-10-21T13:10:03.565+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:10:03.566+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:10:03.566+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:10:03.714+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileOutputCommitter: Saved output of task 'attempt_20241021131003110974861798503606_0024_m_000000_18' to file:/opt/***/datalake/silverzone/kabum_2024-10-21/dim_marca.parquet/_temporary/0/task_20241021131003110974861798503606_0024_m_000000
[2024-10-21T13:10:03.715+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SparkHadoopMapRedUtil: attempt_20241021131003110974861798503606_0024_m_000000_18: Committed. Elapsed time: 33 ms.
[2024-10-21T13:10:03.717+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO Executor: Finished task 0.0 in stage 24.0 (TID 18). 15054 bytes result sent to driver
[2024-10-21T13:10:03.718+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 190 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:03.718+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2024-10-21T13:10:03.719+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: ResultStage 24 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.208 s
[2024-10-21T13:10:03.720+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:10:03.721+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2024-10-21T13:10:03.721+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO DAGScheduler: Job 15 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.213152 s
[2024-10-21T13:10:03.722+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileFormatWriter: Start to commit write Job 0ece5708-4eee-413e-9c03-00afe6b9bd8b.
[2024-10-21T13:10:03.923+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileFormatWriter: Write Job 0ece5708-4eee-413e-9c03-00afe6b9bd8b committed. Elapsed time: 201 ms.
[2024-10-21T13:10:03.924+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileFormatWriter: Finished processing stats for write job 0ece5708-4eee-413e-9c03-00afe6b9bd8b.
[2024-10-21T13:10:03.948+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileSourceStrategy: Pushed Filters:
[2024-10-21T13:10:03.949+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileSourceStrategy: Post-Scan Filters:
[2024-10-21T13:10:03.977+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO CodeGenerator: Code generated in 13.816497 ms
[2024-10-21T13:10:03.979+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 199.7 KiB, free 125.1 MiB)
[2024-10-21T13:10:03.991+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on e2c3a206786d:37869 in memory (size: 44.7 KiB, free: 126.9 MiB)
[2024-10-21T13:10:03.995+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO BlockManagerInfo: Removed broadcast_21_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 126.9 MiB)
[2024-10-21T13:10:03.996+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 125.3 MiB)
[2024-10-21T13:10:03.996+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on e2c3a206786d:37869 (size: 34.3 KiB, free: 126.9 MiB)
[2024-10-21T13:10:03.997+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO SparkContext: Created broadcast 29 from parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:03.997+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4716974 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-21T13:10:04.000+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerInfo: Removed broadcast_19_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 126.9 MiB)
[2024-10-21T13:10:04.004+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Registering RDD 76 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 5
[2024-10-21T13:10:04.006+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Got map stage job 16 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:04.008+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:04.009+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Parents of final stage: List()
[2024-10-21T13:10:04.010+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:04.011+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[76] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:04.012+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 28.6 KiB, free 125.7 MiB)
[2024-10-21T13:10:04.012+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 125.6 MiB)
[2024-10-21T13:10:04.013+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on e2c3a206786d:37869 (size: 13.0 KiB, free: 126.9 MiB)
[2024-10-21T13:10:04.014+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:04.014+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[76] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:04.015+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-10-21T13:10:04.016+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19) (e2c3a206786d, executor driver, partition 0, PROCESS_LOCAL, 9614 bytes)
[2024-10-21T13:10:04.017+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO Executor: Running task 0.0 in stage 25.0 (TID 19)
[2024-10-21T13:10:04.030+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerInfo: Removed broadcast_25_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.0 MiB)
[2024-10-21T13:10:04.039+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on e2c3a206786d:37869 in memory (size: 34.3 KiB, free: 127.0 MiB)
[2024-10-21T13:10:04.040+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO CodeGenerator: Code generated in 24.541638 ms
[2024-10-21T13:10:04.046+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileScanRDD: Reading File path: file:///opt/***/datalake/rawzone/kabum_2024-10-21/data.json, range: 0-522670, partition values: [empty row]
[2024-10-21T13:10:04.048+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerInfo: Removed broadcast_28_piece0 on e2c3a206786d:37869 in memory (size: 100.8 KiB, free: 127.1 MiB)
[2024-10-21T13:10:04.095+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO Executor: Finished task 0.0 in stage 25.0 (TID 19). 2722 bytes result sent to driver
[2024-10-21T13:10:04.096+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 88 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:04.097+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-10-21T13:10:04.098+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: ShuffleMapStage 25 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.095 s
[2024-10-21T13:10:04.099+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: looking for newly runnable stages
[2024-10-21T13:10:04.099+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: running: Set()
[2024-10-21T13:10:04.099+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: waiting: Set()
[2024-10-21T13:10:04.100+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: failed: Set()
[2024-10-21T13:10:04.100+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-10-21T13:10:04.106+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:04.106+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:04.107+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:04.107+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:04.108+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:04.108+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:04.108+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:04.166+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO CodeGenerator: Code generated in 11.450878 ms
[2024-10-21T13:10:04.172+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-10-21T13:10:04.173+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Got job 17 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-10-21T13:10:04.174+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Final stage: ResultStage 27 (parquet at NativeMethodAccessorImpl.java:0)
[2024-10-21T13:10:04.175+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
[2024-10-21T13:10:04.180+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Missing parents: List()
[2024-10-21T13:10:04.180+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[79] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-21T13:10:04.188+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 242.4 KiB, free 126.2 MiB)
[2024-10-21T13:10:04.190+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 89.8 KiB, free 126.1 MiB)
[2024-10-21T13:10:04.191+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on e2c3a206786d:37869 (size: 89.8 KiB, free: 127.0 MiB)
[2024-10-21T13:10:04.191+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2024-10-21T13:10:04.192+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[79] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-10-21T13:10:04.192+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2024-10-21T13:10:04.193+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 20) (e2c3a206786d, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2024-10-21T13:10:04.194+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO Executor: Running task 0.0 in stage 27.0 (TID 20)
[2024-10-21T13:10:04.205+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO ShuffleBlockFetcherIterator: Getting 1 (328.0 B) non-empty blocks including 1 (328.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-10-21T13:10:04.206+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-10-21T13:10:04.219+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO CodeGenerator: Code generated in 13.529482 ms
[2024-10-21T13:10:04.221+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:04.221+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:04.222+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:04.222+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-10-21T13:10:04.223+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-10-21T13:10:04.223+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-21T13:10:04.224+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:10:04.224+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO CodecConfig: Compression: SNAPPY
[2024-10-21T13:10:04.224+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-10-21T13:10:04.225+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-10-21T13:10:04.225+0000] {spark_submit.py:634} INFO - {
[2024-10-21T13:10:04.226+0000] {spark_submit.py:634} INFO - "type" : "struct",
[2024-10-21T13:10:04.226+0000] {spark_submit.py:634} INFO - "fields" : [ {
[2024-10-21T13:10:04.226+0000] {spark_submit.py:634} INFO - "name" : "category",
[2024-10-21T13:10:04.227+0000] {spark_submit.py:634} INFO - "type" : "string",
[2024-10-21T13:10:04.227+0000] {spark_submit.py:634} INFO - "nullable" : true,
[2024-10-21T13:10:04.228+0000] {spark_submit.py:634} INFO - "metadata" : { }
[2024-10-21T13:10:04.228+0000] {spark_submit.py:634} INFO - } ]
[2024-10-21T13:10:04.229+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:10:04.229+0000] {spark_submit.py:634} INFO - and corresponding Parquet message type:
[2024-10-21T13:10:04.229+0000] {spark_submit.py:634} INFO - message spark_schema {
[2024-10-21T13:10:04.230+0000] {spark_submit.py:634} INFO - optional binary category (STRING);
[2024-10-21T13:10:04.230+0000] {spark_submit.py:634} INFO - }
[2024-10-21T13:10:04.230+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:10:04.230+0000] {spark_submit.py:634} INFO - 
[2024-10-21T13:10:04.315+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerInfo: Removed broadcast_30_piece0 on e2c3a206786d:37869 in memory (size: 13.0 KiB, free: 127.0 MiB)
[2024-10-21T13:10:04.384+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileOutputCommitter: Saved output of task 'attempt_202410211310046077661814202998507_0027_m_000000_20' to file:/opt/***/datalake/silverzone/kabum_2024-10-21/dim_categoria.parquet/_temporary/0/task_202410211310046077661814202998507_0027_m_000000
[2024-10-21T13:10:04.385+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SparkHadoopMapRedUtil: attempt_202410211310046077661814202998507_0027_m_000000_20: Committed. Elapsed time: 34 ms.
[2024-10-21T13:10:04.385+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO Executor: Finished task 0.0 in stage 27.0 (TID 20). 6137 bytes result sent to driver
[2024-10-21T13:10:04.387+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 20) in 193 ms on e2c3a206786d (executor driver) (1/1)
[2024-10-21T13:10:04.387+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-10-21T13:10:04.388+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: ResultStage 27 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.210 s
[2024-10-21T13:10:04.389+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-21T13:10:04.389+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-10-21T13:10:04.390+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO DAGScheduler: Job 17 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.216031 s
[2024-10-21T13:10:04.390+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileFormatWriter: Start to commit write Job da6caf2b-00c4-43ed-a516-5eb38a1f9b66.
[2024-10-21T13:10:04.568+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileFormatWriter: Write Job da6caf2b-00c4-43ed-a516-5eb38a1f9b66 committed. Elapsed time: 179 ms.
[2024-10-21T13:10:04.569+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO FileFormatWriter: Finished processing stats for write job da6caf2b-00c4-43ed-a516-5eb38a1f9b66.
[2024-10-21T13:10:04.570+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-10-21T13:10:04.580+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SparkUI: Stopped Spark web UI at http://e2c3a206786d:4040
[2024-10-21T13:10:04.591+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-10-21T13:10:04.604+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO MemoryStore: MemoryStore cleared
[2024-10-21T13:10:04.604+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManager: BlockManager stopped
[2024-10-21T13:10:04.609+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-10-21T13:10:04.611+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-10-21T13:10:04.627+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:04 INFO SparkContext: Successfully stopped SparkContext
[2024-10-21T13:10:05.568+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:05 INFO ShutdownHookManager: Shutdown hook called
[2024-10-21T13:10:05.569+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-02b91f02-a0e8-4951-ad99-271ee92c1f9a
[2024-10-21T13:10:05.572+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5f64e75-cdd3-497f-b59d-e99a2c0a4419/pyspark-cccd7550-0aae-48a3-b1ca-5ca510a694a9
[2024-10-21T13:10:05.576+0000] {spark_submit.py:634} INFO - 24/10/21 13:10:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5f64e75-cdd3-497f-b59d-e99a2c0a4419
[2024-10-21T13:10:05.639+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-10-21T13:10:05.640+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=pipeline_webscraping, task_id=kabum_tasks.Spark_Transformation_kabum, run_id=manual__2024-10-21T13:03:12.581641+00:00, execution_date=20241021T130312, start_date=20241021T130944, end_date=20241021T131005
[2024-10-21T13:10:05.691+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-10-21T13:10:05.742+0000] {taskinstance.py:3900} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2024-10-21T13:10:05.746+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
