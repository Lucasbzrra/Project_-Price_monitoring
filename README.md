# Projeto: Monitoramento de PreÃ§os de Produtos de Hardware

## DescriÃ§Ã£o do Projeto

Este projeto implementa um pipeline de dados funcional usando **Airflow** e a DAG **DagWebScraping** para um processo completo de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga). Ele integra mÃºltiplas fontes de dados e utiliza **Python**, **Selenium**, **BeautifulSoup** e **ProgramaÃ§Ã£o Orientada a Objetos** para automatizar a extraÃ§Ã£o e manipulaÃ§Ã£o de dados sobre produtos de hardware.

## Estrutura do Pipeline de Dados

### ğŸ” Etapa de ExtraÃ§Ã£o

- **Objetivo**: Extrair dados de diversos sites de e-commerce sobre produtos de hardware, como processadores, placas-mÃ£e, memÃ³rias e placas de vÃ­deo.
- **Ferramentas e TÃ©cnicas**:
  - Desenvolvi **operadores personalizados** no Airflow, utilizando `SparkSubmitOperator`, `PythonOperator` e `BashOperator`.
  - A **classe `OperatorSiteBase`** foi criada para padronizar a extraÃ§Ã£o de dados de diferentes fontes.
  - Dados extraÃ­dos sÃ£o salvos em um **Data Lake** com trÃªs camadas:
    - **Raw Zone**: Dados brutos.
    - **Silver Zone**: Dados transformados e limpos.
    - **Gold Zone**: Dados prontos para anÃ¡lise.

### âš™ï¸ Etapa de TransformaÃ§Ã£o

- **Objetivo**: Estruturar os dados em um modelo relacional.
- **Ferramentas e TÃ©cnicas**:
  - UtilizaÃ§Ã£o do **SparkSubmitOperator** para transformar dados da Raw Zone em tabelas de dimensÃµes e fatos.
  - Dados finais sÃ£o armazenados no **Data Lake** usando **Amazon S3** via **MinIO**.

### ğŸ“Š Monitoramento e ValidaÃ§Ã£o

- **Objetivo**: Garantir a integridade dos dados e o sucesso das operaÃ§Ãµes do pipeline.
- **Ferramentas e TÃ©cnicas**:
  - **Sensores** monitoram o status da Silver Zone no Airflow.
  - **BranchOperator** valida condiÃ§Ãµes especÃ­ficas e envia alertas por e-mail se necessÃ¡rio.

### ğŸ“¥ Etapa de Carga

- **Objetivo**: Carregar dados transformados no PostgreSQL.
- **Ferramentas e TÃ©cnicas**:
  - Uma DAG dependente conecta-se ao banco de dados **PostgreSQL**.
  - Cria tabelas, relacionamentos e insere dados usando o operador Spark.

## Tecnologias Utilizadas

- **Docker**: OrquestraÃ§Ã£o dos containers de Airflow, Spark, PostgreSQL e MinIO.
- **Airflow**: OrquestraÃ§Ã£o e agendamento de tarefas ETL.
- **Python**: Desenvolvimento de operadores e classes personalizadas.
- **Spark**: TransformaÃ§Ã£o de dados e estruturaÃ§Ã£o.
- **PostgreSQL**: Armazenamento de dados transformados.
- **MinIO**: SimulaÃ§Ã£o do S3 para o Data Lake.

## Estrutura do Projeto

```plaintext
â”œâ”€â”€ dags/
|   â”œâ”€â”€ LoadWebScraping.py
â”‚   â””â”€â”€ DagWebScraping.py          
â”œâ”€â”€ operators/
|       â”œâ”€â”€OperatorBase.py
â”‚       â”œâ”€â”€ OperatorSiteA.py
|       â”œâ”€â”€ OperatorSiteB.py
|       â”œâ”€â”€ OperatorFileSensor.py       
â”‚       â””â”€â”€ OperatorSpark.py      
â”œâ”€â”€ src/
â”‚   â””â”€â”€ scripts_spark/
â”‚       â”œâ”€â”€ LoadFileToDB.py       
â”‚       â””â”€â”€ WebScraping_Transformation.py 
â”œâ”€â”€ minio_data/
â”‚   â”œâ”€â”€ rawzone/                   
â”‚   â”œâ”€â”€ silver/                    
â”‚   â””â”€â”€ gold/                      
â”œâ”€â”€ docker-compose.yaml            
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.spark                     
â””â”€â”€ README.md                      
